{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "make_embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsTtBNgQcKn1e6/juPeUnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathanbollig/machine-mutation/blob/main/make_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRkqeiEpN8PA",
        "outputId": "862343ea-930c-44b8-f618-8d7708ecd5d4"
      },
      "source": [
        "!pip install torch\n",
        "!pip install git+https://github.com/facebookresearch/esm.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting git+https://github.com/facebookresearch/esm.git\n",
            "  Cloning https://github.com/facebookresearch/esm.git to /tmp/pip-req-build-v0u204d5\n",
            "  Running command git clone -q https://github.com/facebookresearch/esm.git /tmp/pip-req-build-v0u204d5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: esm\n",
            "  Building wheel for esm (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for esm: filename=esm-0.3.0-cp37-none-any.whl size=35727 sha256=dce5337c06f8ebf064d241c0c58beb96648fea29afbfeefa7941286a57a03246\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_hmigmby/wheels/68/f1/02/8c8e4fea902cc926d482415aa393f6dc1eb1a41d2d78e1ec75\n",
            "Successfully built esm\n",
            "Installing collected packages: esm\n",
            "Successfully installed esm-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14_bJTTVOQeM",
        "outputId": "4f3804a0-77fd-4c04-cbb4-9df5d7d361f5"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/esm.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'esm'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 176 (delta 100), reused 121 (delta 47), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (176/176), 861.31 KiB | 14.60 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eakkED42UXGC"
      },
      "source": [
        "Need to move the Sequences.fasta file to content/esm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJzIVFqlPac1",
        "outputId": "a060c79f-78c6-43dc-9408-182887d662f1"
      },
      "source": [
        "!python esm/extract.py esm1b_t33_650M_UR50S esm/Sequences.fasta output/ --repr_layers 0 32 33 --include mean per_tok"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 2609438720 bytes == 0x5579d6118000 @  0x7fd6ead80b6b 0x7fd6eada0379 0x7fd69747d25e 0x7fd69747e9d2 0x7fd6d4d22495 0x7fd6e5f375f9 0x5579368d7c25 0x5579368987f2 0x55793690bd75 0x557936906e0d 0x55793689938b 0x557936898e99 0x5579369e070d 0x55793694f57b 0x557936897f41 0x55793698999d 0x55793690bfe9 0x557936906e0d 0x5579367d8e2b 0x5579369091e6 0x557936906b0e 0x55793689977a 0x55793690886a 0x557936906b0e 0x55793689977a 0x55793690886a 0x55793689969a 0x557936907a45 0x55793689969a 0x557936907a45 0x55793689969a\n",
            "tcmalloc: large alloc 2609438720 bytes == 0x557a719a6000 @  0x7fd6ead80b6b 0x7fd6eada0379 0x7fd69747d25e 0x7fd69747e9d2 0x7fd6d4d22495 0x7fd6e5f375f9 0x5579368d7c25 0x5579368987f2 0x55793690bd75 0x557936906e0d 0x55793689938b 0x557936898e99 0x5579369e070d 0x55793694f57b 0x557936897f41 0x55793698999d 0x55793690bfe9 0x557936906e0d 0x5579367d8e2b 0x5579369091e6 0x557936906b0e 0x55793689977a 0x55793690886a 0x557936906b0e 0x55793689977a 0x55793690886a 0x55793689969a 0x557936907a45 0x55793689969a 0x557936907a45 0x55793689969a\n",
            "Transferred model to GPU\n",
            "Read esm/Sequences.fasta with 1238 sequences\n",
            "Processing 1 of 1238 batches (1 sequences)\n",
            "Traceback (most recent call last):\n",
            "  File \"esm/extract.py\", line 134, in <module>\n",
            "    main(args)\n",
            "  File \"esm/extract.py\", line 91, in main\n",
            "    out = model(toks, repr_layers=repr_layers, return_contacts=return_contacts)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/esm/esm/model.py\", line 131, in forward\n",
            "    x = x + self.embed_positions(tokens)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/esm/esm/modules.py\", line 225, in forward\n",
            "    f'Sequence length {input.size(1)} above maximum '\n",
            "ValueError: Sequence length 2398 above maximum  sequence length of 1024\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}